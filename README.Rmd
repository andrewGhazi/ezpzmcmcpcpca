---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# ezpzmcmcpcpca

R implementation of PCPCA. PCPCA is the work of Didong Li, Andrew Jones, and Barbara Engelhardt. Original manuscript here: https://arxiv.org/abs/2012.07977 and original Python implementation here: https://github.com/andrewcharlesjones/pcpca

The MCMC implementation here is derived from the original Stan code but uses a few linear algebra identities  that make it comparatively fast (Woodbury matrix inverse + matrix determinant lemma) and interpretable (Haar distribution on contrastive axes to ensure they're orthogonal).

<!-- badges: start -->
<!-- badges: end -->

## Installation

This package depends on CmdStan & cmdstanr, which are not available via the usual CRAN route. 

```{r eval = FALSE}

install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

library(cmdstanr)

check_cmdstan_toolchain()

install_cmdstan(cores = 2)

remotes::install_github("andrewGhazi/ezpzmcmcpcpca")
```

If the `cmdstanr` installation doesn't work you can find more detailed instructions [at this link](https://mc-stan.org/cmdstanr/). 

## Example

The package has two functions `pcpca_mle()` and `pcpca_mcmc()`. The former directly implements the MLE estimates from the paper. The second adapts the Gibbs posterior provided in the original python implementation. 

```{r example}
library(ezpzmcmcpcpca)

set.seed(123)
mu_vec = c(-.75, .75)
Sigma = matrix(nrow = 2, c(1, .7, .7, 1))

X1 = MASS::mvrnorm(100, 
                   mu = mu_vec,
                   Sigma = Sigma)

X2 = MASS::mvrnorm(100, 
                   mu = -1 * mu_vec,
                   Sigma = Sigma)

Y = MASS::mvrnorm(200, 
                  mu = rep(0,2),
                  Sigma = Sigma)
X = rbind(X1, X2)

plot(rbind(Y, X1, X2),
     pch = 19, 
     col = c(rep('grey', 200), 
             rep('dodgerblue2', 100), 
             rep('firebrick1', 100)),
     xlab = 'dim_1', ylab = 'dim_2',
     main = "Contrastive axis on 2D data")

mle_estimate = pcpca_mle(X,Y, gamma = .85)

# $sigma2_mle
# [1] 0.01181884
# 
# $W_mle
#            [,1]
# [1,] -0.1142008
# [2,]  0.1195985

mcmc_estimate = pcpca_mcmc(X, Y, .85, d = 1,
                           mle_est = mle_estimate, 
                           refresh = 0)

W_draws = mcmc_estimate$draws('W', format = 'matrix')[sample.int(4000,40),]

# > mcmc_estimate
#   variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail
#  lp__      -42.85 -42.51 1.31 1.07 -45.41 -41.43 1.00     1269     1860
#  W[1,1]      2.15   2.12 0.30 0.29   1.71   2.70 1.00     1406     1312
#  W[2,1]     -1.98  -1.95 0.29 0.27  -2.50  -1.57 1.00     1384     1329
#  sigma2      0.24   0.23 0.07 0.07   0.15   0.38 1.00     2024     1901
#  ...

arrows(x0 = 0, y0 = 0,
       x1 = mle_estimate$W_mle[1,1],
       y1 = mle_estimate$W_mle[2,1], 
       col = "limegreen", length = .075, lwd = 3)
arrows(x0 = 0, y0 = 0,
       x1 = W_draws[,1],
       y1 = W_draws[,2], 
       col = rgb(0,0,0,.333), length = .1)
```


# Identifiability 

It also works for higher numbers of latent dimensions. This simulation explicitly defines two contrastive axes (`W_rand`), but one could also simply create multiple groups in X with varying mean. 

A note on identifiability. The sign of the contrastive axes are not identifiable. Axes pointing like this `_|` have the same posterior density as axes pointing like this `Î“`. 

To help get around this, we produce a generated quantity variable `W_id` that dots each axis onto the MLE, and if it's negative multiplies by -1. This works well if there is sufficient data, and the convergence metrics of `W_id` (particularly Rhat) will be happy. Higher numbers of latent dimensions requires more data for clear inference. 

Nonetheless, this doesn't always work. If you notice component(s) of `W_id` that seem to be symmetrically flipping about zero, that means there's substantial probability that the posterior mean isn't pointing in the right direction. This diagnostic use is one of the main advantages of the MCMC approach.

```{r}
library(ezpzmcmcpcpca)

set.seed(123)
p = 10
k = 2

# more variance along the second axis
W_rand = matrix(rnorm(p*k), ncol = k) %*% diag(c(2,.75)); W_rand

cor_mat = trialr::rlkjcorr(1, p, 1);

Y = MASS::mvrnorm(n = 500, mu = rep(0,p),
                  Sigma = cor_mat)

contrastive_noise = MASS::mvrnorm(n = 500,
                                  Sigma = diag(k),
                                  mu = rep(0,k))

X = MASS::mvrnorm(n = 500, Sigma = cor_mat,
                  mu = rep(0,p)) +
  (contrastive_noise %*% t(W_rand))


res_mle = pcpca_mle(X, Y, .85, 2); res_mle
res_mcmc = pcpca_mcmc(X, Y, .85, 2, 
                      chains = 4,
                      parallel_chains = 4, refresh = 0,
                      show_messages = FALSE)

res_mcmc$summary(c("sigma2", "W_id"))

# Look at the first component
res_mcmc$draws('W_id', format = 'matrix')[,1:10] |>
  bayesplot::mcmc_trace() 

contrastive_magnitude = sqrt(rowSums(contrastive_noise^2))
pal_fun = colorRampPalette(c('grey', 'red'))

contrast_cols = pal_fun(20)[cut((contrastive_magnitude / max(contrastive_magnitude)),
                                breaks = 20) |> as.numeric()]

rbind(X) %*% matrix(res_mcmc$summary("W_id")$mean, ncol = 2) |> 
  plot(xlab = "", ylab = "",
       main = "10D data projected to first two contrastive axes",
       sub = "Points colored by magnitude of true contrastive noise",
       col = contrast_cols, 
       pch = 19)

plot(W_rand[1:(p*k)], matrix(res_mcmc$summary("W_id")$mean, ncol = 1), 
     pch = 19, 
     xlab = 'true axis components', ylab = 'posterior means', 
     main = "True vs estimated axis components")

```

